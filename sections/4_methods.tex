\renewcommand{\thefootnote}{\alph{footnote}} % Use letters for footnotes

\section{Methods}

All modelling and analysis was performed using Python in Jupyter Notebooks \cite{kluyver_jupyter_2016}, with general analysis and plotting performed using NumPy \cite{harris_array_2020}, Pandas \cite{mckinney-proc-scipy-2010}, Scikit-Learn  \cite{pedregosa_scikit-learn_2011}, and Matplotlib \cite{hunter_matplotlib_2007}. 

Further details of methods may also be found in the supplementary material. 

All code, with detailed results, used is available online at \url{https://samuel-book.github.io/samuel_shap_paper_1/} (DOI: 10.5281/zenodo.7540391). 

\subsection{Data}

Data was retrieved for 246,676 emergency stroke admissions to \kpFIXME{the 132} acute stroke teams in England and Wales between 2016 and 2018 (three full years), obtained from the Sentinel Stroke National Audit (SSNAP \footnote{https://www.strokeaudit.org/}). Data fields are provided for the acute phase of the stroke pathway (full details of the data fields obtained are provided in the appendix). 88,928 of these patients arrived within 4 hours of known stroke onset\kpFIXME{, and were used in this modelling study (as they represent the patients that had time left to treat)}. No identifiable patient or hospital information was obtained, and anonymised hospital names were provided.

 SSNAP has near-complete coverage of all acute stroke admissions in the UK (outside Scotland). All hospitals admitting acute stroke participate in the audit, and year-on-year comparison with Hospital Episode Statistics \footnote{https://digital.nhs.uk/data-and-information/data-tools-and-services/data-services/hospital-episode-statistics} confirms estimated case ascertainment of 95\% of coded cases of acute stroke.

The NHS Health Research Authority decision tool \footnote{http://www.hra-decisiontools.org.uk/research/} was used to confirm that ethical approval was not required to access the data. Data access was authorised by the Healthcare Quality Improvement Partnership (HQIP)\footnote{https://www.hqip.org.uk/} (reference HQIP303). 

\subsection{Machine learning models (to predict thromboylsis use)}

We used an \emph{eXtreme Gradient Boosting model \cite{chen_xgboost_2016}} (`XGBoost`) to predict the probability of use of thrombolysis for each patient. A single model was fitted for all hospitals, with hospital attended being one patient feature.

Features were selected from the 60 original features by forward-feature selection, selecting features one at a time by which feature gives most improvement in accuracy as measured by Receiver Operating Characteristic (ROC) Area Under Curve (AUC). Model accuracy was tested using 5-fold cross-validation.

\subsection{SHapley Additive exPlanation values}

SHAP provides a measure of the contribution of each patient feature value to the final predicted probability of receiving thrombolysis. For calculation of SHAP values we used the Shap library \cite{lundberg_unified_2017}.

\subsection{Comparing hospital decision making}
\subsubsection{Method 1: 10k patient cohort}
\kpFIXME{KP comment: I edited this paragraph, I got confused with "comparing thrombolysis use between methods". Sorry if I misunderstood and this now does not represent what you were trying to say}
To compare thrombolysis use between hospitals, we separated out 10k patients (chosen at random) which were held back from the training phase. The model was trained on the remaining 78,928 real patients (with these patients attending their actual hospital). The use of thrombolysis at each hospital was then predicted for the same 10k cohort of patients (those that were held back from the training phase) by passing all of the 10k patients though the model whilst setting the hospital ID to each hospital in turn.

\subsubsection{Method 2: Artificial patients}
\kpFIXME{KP comment: I edited this paragraph, I got confused with "comparing thrombolysis use between methods". Sorry if I misunderstood and this now does not represent what you were trying to say}
Another method to compare thrombolysis use between hospitals, we constructed artificial patients and passed those through the model for each hospital. When investigating artificial patients, we trained the model on all of the 88,928 real patients. 